import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from tqdm import tqdm
import traceback

from torch.utils.data import Dataset, DataLoader
from torchkeras.tabular import TabularPreprocessor, TabularDataset
from torchkeras.tabular.models import FTTransformerConfig, FTTransformerModel

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from src.imbalance_data_processing import apply_smote, apply_under_sampling
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score
from sklearn.model_selection import KFold
from sklearn.utils.class_weight import compute_class_weight

np.random.seed(42)

# ----- æ•¸å€¼å‹èˆ‡é¡åˆ¥å‹ç‰¹å¾µ -----
NUMERICAL_FEATURES = ['age', 'num_procedures', 'days_in_hospital', 'comorbidity_score']
CATEGORICAL_FEATURES = ['gender', 'primary_diagnosis', 'discharge_to']
TARGET = 'readmitted'
NUMERICAL_COLUMNS = [f'num_{i}' for i in range(len(NUMERICAL_FEATURES))]
CATEGORICAL_COLUMNS = [f'cat_{i}' for i in range(len(CATEGORICAL_FEATURES))]

class CustomerFTTransformer:
    def __init__(self, num_attn_blocks=1, dropout=0.1):
        self.num_attn_blocks = num_attn_blocks
        self.dropout = dropout

        # è®€å–è¨“ç·´æ•¸æ“š
        self.train_df = pd.read_csv("data/train_df.csv")

        # å°è¨“ç·´æ•¸æ“šé€²è¡Œshuffleï¼Œç¢ºä¿æ•¸æ“šéš¨æ©Ÿæ€§
        self.train_df = self.train_df.sample(frac=1, random_state=42).reset_index(drop=True)
        self.test_df_test_csv = pd.read_csv("data/test_df.csv")

        self.scaler = StandardScaler()
        # ä½¿ç”¨ TabularPreprocessor é€²è¡Œé è™•ç†
        self.preprocessor = TabularPreprocessor(
            cat_features=CATEGORICAL_COLUMNS,  # é¡åˆ¥ç‰¹å¾µ
            numeric_features=NUMERICAL_COLUMNS,    # æ•¸å€¼ç‰¹å¾µ
            normalization='standard',           # æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆå¯é¸ï¼š'minmax' æˆ– Noneï¼‰
            onehot_max_cat_num=1,
        )

        self.model_config = FTTransformerConfig(
            # ModelConfig åƒæ•¸
            task="classification",  # äºŒå…ƒåˆ†é¡
            num_attn_blocks=self.num_attn_blocks,  # ä½¿ç”¨å¯¦ä¾‹è®Šé‡
            ff_dropout=self.dropout,
            attn_dropout=self.dropout,
            embedding_dropout=self.dropout,
            add_norm_dropout=self.dropout,
        )

        self.model = None
        self.config = None
        self.train_df_train_csv_processed = None
        self.test_df_train_csv_processed = None
        self.train_target_df = None
        self.test_target_df = None
        self.train_df_train_csv = None
        self.test_df_train_csv = None
        self.numerical_features_processed = None
        self.embedding_features_processed = None
        self.ds_train = None
        self.ds_test = None
        self.dl_train = None
        self.dl_test = None

    def format_dataframe(self):
        # å°‡é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ç‚ºæ•¸å€¼ï¼ˆSMOTENC éœ€è¦æ•´æ•¸å½¢å¼çš„é¡åˆ¥ç‰¹å¾µï¼‰
        self.label_encoders = {}
        
        for col in CATEGORICAL_FEATURES:
            le = LabelEncoder()
            self.train_df[col] = le.fit_transform(self.train_df[col])
            self.label_encoders[col] = le
            
            # è½‰æ›ç‚º category é¡å‹
            self.train_df[col] = self.train_df[col].astype('category')
            
            print(f"Encoded {col}: {self.train_df[col].cat.categories.tolist()}")

        self.train_df[TARGET] = self.train_df[TARGET].astype('category')
        
        print("=== Feature Encoding Complete ===")
        print(f"Numerical features: {NUMERICAL_FEATURES}")
        print(f"Categorical features (encoded): {CATEGORICAL_FEATURES}")
        
        # æª¢æŸ¥ç·¨ç¢¼å¾Œçš„æ•¸æ“šæ¨£æœ¬
        print("\nSample of encoded data:")
        print(self.train_df[CATEGORICAL_FEATURES].head())

    def _split_train_test(self):
        # è½‰æˆ numpy
        X_num = self.train_df[NUMERICAL_FEATURES].values.astype(np.float32)
        X_cat = self.train_df[CATEGORICAL_FEATURES].values
        y = self.train_df[TARGET].values.astype(np.int64)

        # åˆ†å‰²è¨“ç·´æ¸¬è©¦è³‡æ–™
        return train_test_split(
            X_num, X_cat, y, test_size=0.2, random_state=42
        )

    def preprocess(self, use_smote=False, use_under_sampling=False, smote_method='smotenc', under_sampling_method='tomek'):
        X_num_train, X_num_test, X_cat_train, X_cat_test, y_train, y_test = self._split_train_test()
        
        # ä¿å­˜åŸå§‹çš„ y_train ç”¨æ–¼ class weight è¨ˆç®—
        self.original_y_train = y_train.copy()
        
        # æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µï¼ˆé€™å° SMOTE å¾ˆé‡è¦ï¼‰
        X_num_train = self.scaler.fit_transform(X_num_train)
        X_num_test = self.scaler.transform(X_num_test)

        # æª¢æŸ¥æ¡æ¨£æ–¹æ³•è¡çª
        if use_smote and use_under_sampling:
            print(f"Applying Under Sampling using: {under_sampling_method}, then applying SMOTE using: {smote_method}")

        # Apply Under Sampling if requested
        if use_under_sampling:
            print(f"ğŸ”„ Applying under sampling with method: {under_sampling_method}")
            X_num_train_orig_size = len(X_num_train)
            X_num_train, X_cat_train, y_train = apply_under_sampling(X_num_train, X_cat_train, y_train, method=under_sampling_method)
            print(f"After under sampling - Training set size: {len(y_train)} (was {X_num_train_orig_size})")

        # Apply SMOTE if requested (and not using under sampling)
        if use_smote:
            print(f"ğŸ”„ Applying SMOTE with method: {smote_method}")
            X_num_train_orig_size = len(X_num_train)
            X_num_train, X_cat_train, y_train = apply_smote(X_num_train, X_cat_train, y_train, method=smote_method)
            print(f"After SMOTE - Training set size: {len(y_train)} (was {X_num_train_orig_size})")
            print(f"After SMOTE - Class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}")
            print(f"After SMOTE - X_num_train shape: {X_num_train.shape}")
            print(f"After SMOTE - X_cat_train shape: {X_cat_train.shape}")

        train_df_train_csv = pd.DataFrame({
            **{f'num_{i}': X_num_train[:, i] for i in range(X_num_train.shape[1])},
            **{f'cat_{i}': X_cat_train[:, i] for i in range(X_cat_train.shape[1])},
            'target': y_train
        })

        print(f"train_df_train_csv shape after DataFrame creation: {train_df_train_csv.shape}")

        test_df_train_csv = pd.DataFrame({
            **{f'num_{i}': X_num_test[:, i] for i in range(X_num_test.shape[1])},
            **{f'cat_{i}': X_cat_test[:, i] for i in range(X_cat_test.shape[1])},
            'target': y_test
        })

        self.train_target_df = train_df_train_csv['target']
        self.test_target_df = test_df_train_csv['target']

        self.train_df_train_csv = train_df_train_csv.drop(columns=['target'])
        self.test_df_train_csv = test_df_train_csv.drop(columns=['target'])

        self.preprocessor.fit(train_df_train_csv)
        train_df_train_csv_processed = self.preprocessor.transform(train_df_train_csv)
        test_df_train_csv_processed = self.preprocessor.transform(test_df_train_csv)

        self.train_df_train_csv_processed = train_df_train_csv_processed.loc[:, ~train_df_train_csv_processed.columns.duplicated(keep='last')]
        self.test_df_train_csv_processed = test_df_train_csv_processed.loc[:, ~test_df_train_csv_processed.columns.duplicated(keep='last')]

        print(f"Final train_df_train_csv_processed shape: {self.train_df_train_csv_processed.shape}")
        print(f"Final test_df_train_csv_processed shape: {self.test_df_train_csv_processed.shape}")
        print(f"=== END PREPROCESS DEBUG ===")

    # æª¢æŸ¥é¡åˆ¥åˆ†ä½ˆ
    def check_class_distribution(self, df:pd.DataFrame, target_col:str):
        """
        Check and print the class distribution of the target variable.
        
        Args:
            df (pd.DataFrame): Input dataframe to check
            target_col (str): Name of target column to analyze
            
        Returns:
            tuple: (value_counts, imbalance_ratio)
        """
        print("=== Class Distribution Analysis ===")
        
        # Get class distribution
        class_counts = df[target_col].value_counts().sort_index()
        print(f"Class distribution:")
        for class_val, count in class_counts.items():
            percentage = (count / len(df)) * 100
            print(f"  Class {class_val}: {count} samples ({percentage:.2f}%)")
        
        # Calculate imbalance ratio
        min_class = class_counts.min()
        max_class = class_counts.max()
        imbalance_ratio = max_class / min_class
        print(f"Imbalance ratio: {imbalance_ratio:.2f}")
        
        if imbalance_ratio > 2.0:
            print("âš ï¸  Dataset is imbalanced.")
        else:
            print("âœ… Dataset is relatively balanced.")
        
        return class_counts, imbalance_ratio

    # è¨­å®šç‰¹å¾µè™•ç†
    def set_feautres_processed(self):
        self.numerical_features_processed = self.preprocessor.get_numeric_features()
        self.embedding_features_processed = self.preprocessor.get_embedding_features()
        
        # å°‡è³‡æ–™èˆ‡ç›®æ¨™è®Šæ•¸åˆä½µå¾Œé€²è¡Œ shuffle
        self.train_df_train_csv_processed['target'] = self.train_target_df
        self.test_df_train_csv_processed['target'] = self.test_target_df
        
        # å°è¨“ç·´è³‡æ–™é€²è¡Œ shuffle
        self.train_df_train_csv_processed = self.train_df_train_csv_processed.sample(
            frac=1, random_state=42
        ).reset_index(drop=True)
        
        # å°æ¸¬è©¦è³‡æ–™é€²è¡Œ shuffle 
        self.test_df_train_csv_processed = self.test_df_train_csv_processed.sample(
            frac=1, random_state=42
        ).reset_index(drop=True)

    # è¨­å®š TabularDataset
    def set_tablar_dataset(self):
        # === å»ºç«‹ TabularDataset ===
        self.ds_train = TabularDataset(
            data=self.train_df_train_csv_processed,
            task='classification',
            target=['target'],
            continuous_cols=self.numerical_features_processed,
            categorical_cols=self.embedding_features_processed
        )

        self.ds_test = TabularDataset(
            data=self.test_df_train_csv_processed,
            task='classification',
            target=['target'],
            continuous_cols=self.numerical_features_processed,
            categorical_cols=self.embedding_features_processed
        )

        self.dl_train = DataLoader(self.ds_train, batch_size=256, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)
        self.dl_test = DataLoader(self.ds_test, batch_size=256, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)

        print("categorical features (cols):", self.embedding_features_processed)
        print("categorical features (array):", self.train_df_train_csv_processed[self.embedding_features_processed].shape[1])
        print("numerical features (cols):", self.numerical_features_processed)
        print("numerical features (array):", self.train_df_train_csv_processed[self.numerical_features_processed].shape[1])

    # è¨­å®šæ¨¡å‹è¨­å®š
    def set_model_config(self):
        # å»ºç«‹æ¨¡å‹
        self.config = self.model_config.merge_dataset_config(self.ds_train)
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = FTTransformerModel(self.config)

    # è¨“ç·´æ¨¡å‹
    def train_model(self, num_epochs: int, model_name: str, use_class_weight=False, plot_train_metrics=True,
                   enable_cv=False, k_folds=3, save_best_model=True, early_stopping=True, patience=10, min_delta=1e-4):
        """
        è¨“ç·´ FT-Transformer æ¨¡å‹ï¼ˆæ”¯æŒ Cross Validationï¼‰
        
        Args:
            num_epochs (int): è¨“ç·´è¼ªæ•¸
            model_name (str): æ¨¡å‹ä¿å­˜åç¨±
            use_class_weight (bool): æ˜¯å¦ä½¿ç”¨ class weight ä¾†è™•ç†é¡åˆ¥ä¸å¹³è¡¡
            plot_train_metrics (bool): æ˜¯å¦åœ¨è¨“ç·´å¾Œç¹ªè£½è¨“ç·´é›†çš„ROCå’Œæ··æ·†çŸ©é™£
            enable_cv (bool): æ˜¯å¦å•Ÿç”¨ Cross Validation
            k_folds (int): K-fold CV çš„ fold æ•¸é‡ï¼ˆç•¶ enable_cv=True æ™‚æœ‰æ•ˆï¼‰
            save_best_model (bool): æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆCVæ¨¡å¼ä¸‹ï¼‰
            early_stopping (bool): æ˜¯å¦å•Ÿç”¨ early stopping
            patience (int): early stopping çš„è€å¿ƒå€¼ï¼ˆå¤šå°‘å€‹ epoch æ²’æœ‰æ”¹å–„å°±åœæ­¢ï¼‰
            min_delta (float): æœ€å°æ”¹å–„é–¾å€¼
            
        Returns:
            dict: è¨“ç·´çµæœï¼ˆåŒ…å«CVçµæœå¦‚æœå•Ÿç”¨çš„è©±ï¼‰
        """
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # é¡¯ç¤ºGPUä¿¡æ¯
        if torch.cuda.is_available():
            print(f"ğŸ–¥ï¸ GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ”¢ å¯ç”¨GPUæ•¸é‡: {torch.cuda.device_count()}")
            print(f"ğŸ’¾ GPUè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            if torch.cuda.device_count() > 1:
                print(f"ğŸš€ å°‡ä½¿ç”¨ {torch.cuda.device_count()} å€‹GPUé€²è¡Œè¨“ç·´")
        else:
            print("âš ï¸ æœªæª¢æ¸¬åˆ°CUDAè¨­å‚™ï¼Œå°‡ä½¿ç”¨CPUè¨“ç·´")
        
        if not enable_cv:
            # åŸæœ‰çš„å¸¸è¦è¨“ç·´æ¨¡å¼
            self._train_single_model(num_epochs, model_name, use_class_weight, 
                                          plot_train_metrics, device, early_stopping, patience, min_delta)
        else:
            # Cross Validation æ¨¡å¼
            self._train_with_cross_validation(num_epochs, model_name, use_class_weight,
                                                   k_folds, save_best_model, device, early_stopping, patience, min_delta)
    
    # å–®ä¸€æ¨¡å‹è¨“ç·´
    def _train_single_model(self, num_epochs, model_name, use_class_weight, plot_train_metrics, device, early_stopping, patience, min_delta):
        """
        åŸæœ‰çš„å–®ä¸€æ¨¡å‹è¨“ç·´æ–¹æ³•
        """
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        
        # è¨ˆç®—ä¸¦è¨­å®š class weights
        if use_class_weight:
            class_weight_tensor = self._compute_class_weights(device)
            criterion = torch.nn.CrossEntropyLoss(weight=class_weight_tensor)
            print(f"Using class weights: {class_weight_tensor.cpu().numpy()}")
        else:
            criterion = torch.nn.CrossEntropyLoss()
            print("Using standard CrossEntropyLoss (no class weights)")

        self.model.to(device)
        
        # å¤šGPUæ”¯æŒ
        if torch.cuda.device_count() > 1:
            print(f"ğŸ”— Using {torch.cuda.device_count()} GPUs for training")
            self.model = torch.nn.DataParallel(self.model)
        
        self.model.train()

        progress_bar = tqdm(range(num_epochs), leave=False)
        for epoch in progress_bar:
            total_loss = 0
            for batch in self.dl_train:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                optimizer.zero_grad()
                outputs = self.model({'continuous': numerical, 'categorical': categorical})
                logits = outputs['logits']
                loss = criterion(logits, target)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            progress_bar.set_description(f"Epoch {epoch+1}/{num_epochs} | Avg. Loss: {total_loss/len(self.dl_train):.4f}")
        
        # å„²å­˜æ¨¡å‹ï¼ˆå»ºè­°ç”¨ .pt æˆ– .pthï¼‰
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        torch.save(model_to_save.state_dict(), f'./models/{model_name}')
        print(f"Model saved as ./models/{model_name}")
        
        # ç¹ªè£½è¨“ç·´é›†è©•ä¼°æŒ‡æ¨™
        if plot_train_metrics:
            print("\n=== Training Set Evaluation ===")
            self.evaluate_train_set(model_name=model_name)
            
        return {"model_name": model_name, "training_completed": True}
    
    # äº¤å‰é©—è­‰è¨“ç·´
    def _train_with_cross_validation(self, num_epochs, model_name, use_class_weight, k_folds, save_best_model, device, early_stopping, patience, min_delta):
        """
        åŸ·è¡Œ K-fold Cross Validation è¨“ç·´
        """
        
        print(f"\nğŸ”„ é–‹å§‹ {k_folds}-Fold Cross Validation è¨“ç·´...")
        
        # æº–å‚™å®Œæ•´çš„è¨“ç·´æ•¸æ“š
        X_train_full = self.train_df_train_csv_processed.values
        y_train_full = self.train_target_df.values
        
        # åˆå§‹åŒ– KFold
        kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        
        # å„²å­˜æ¯å€‹ fold çš„çµæœ
        cv_results = {
            'fold_accuracies': [],
            'fold_losses': [],
            'fold_models': [],
            'fold_metrics': [],
            'mean_accuracy': 0.0,
            'std_accuracy': 0.0,
            'mean_loss': 0.0,
            'std_loss': 0.0
        }
        
        best_accuracy = 0.0
        best_model_state = None
        best_fold = -1
        
        # åŸ·è¡Œ K-fold CV
        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_full)):
            print(f"\nğŸ“‹ Fold {fold + 1}/{k_folds}")
            print("-" * 50)
            
            # åˆ†å‰²ç•¶å‰ fold çš„æ•¸æ“š
            X_train_fold = X_train_full[train_idx]
            X_val_fold = X_train_full[val_idx]
            y_train_fold = y_train_full[train_idx]
            y_val_fold = y_train_full[val_idx]
            
            # å‰µå»ºç•¶å‰ fold çš„ DataLoader
            train_dl_fold, val_dl_fold = self._create_fold_dataloaders(
                X_train_fold, X_val_fold, y_train_fold, y_val_fold
            )
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ¯å€‹ fold ä½¿ç”¨æ–°çš„æ¨¡å‹ï¼‰
            model_fold = FTTransformerModel(config=self.config)
            model_fold.to(device)

            # è¨“ç·´ç•¶å‰ fold çš„æ¨¡å‹
            fold_accuracy, fold_loss, model_state = self._train_fold(
                model_fold, train_dl_fold, val_dl_fold, num_epochs, 
                use_class_weight, device, fold + 1, early_stopping, patience, min_delta
            )
            
            # ä¿å­˜çµæœ
            cv_results['fold_accuracies'].append(fold_accuracy)
            cv_results['fold_losses'].append(fold_loss)
            cv_results['fold_models'].append(model_state)
            
            # è©³ç´°è©•ä¼°ç•¶å‰ fold
            fold_metrics = self._evaluate_fold(model_fold, val_dl_fold, device)
            cv_results['fold_metrics'].append(fold_metrics)
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            if fold_accuracy > best_accuracy:
                best_accuracy = fold_accuracy
                best_model_state = model_state
                best_fold = fold + 1
            
            print(f"Fold {fold + 1} - Validation Accuracy: {fold_accuracy:.4f}, Loss: {fold_loss:.4f}")
        
        # è¨ˆç®— CV çµ±è¨ˆçµæœ
        cv_results['mean_accuracy'] = np.mean(cv_results['fold_accuracies'])
        cv_results['std_accuracy'] = np.std(cv_results['fold_accuracies'])
        cv_results['mean_loss'] = np.mean(cv_results['fold_losses'])
        cv_results['std_loss'] = np.std(cv_results['fold_losses'])
        cv_results['best_fold'] = best_fold
        cv_results['best_accuracy'] = best_accuracy
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if save_best_model and best_model_state is not None:
            # è¼‰å…¥æœ€ä½³æ¨¡å‹åˆ°ç•¶å‰çš„instance
            self.model.load_state_dict(best_model_state)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(best_model_state, f'./models/{model_name}_best_cv.pth')
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: ./models/{model_name}_best_cv.pth (ä¾†è‡ª Fold {best_fold})")
            
        # è¼¸å‡ºCVç¸½çµ
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Cross Validation ç¸½çµ")
        print(f"{'='*60}")
        print(f"å¹³å‡æº–ç¢ºç‡: {cv_results['mean_accuracy']:.4f} Â± {cv_results['std_accuracy']:.4f}")
        print(f"å¹³å‡æå¤±: {cv_results['mean_loss']:.4f} Â± {cv_results['std_loss']:.4f}")
        print(f"æœ€ä½³æ¨¡å‹ä¾†è‡ª: Fold {best_fold} (æº–ç¢ºç‡: {best_accuracy:.4f})")
        print(f"{'='*60}")
        
        cv_results['cv_completed'] = True


        # ç¹ªè£½æ¯å€‹foldçš„æ··æ·†çŸ©é™£
        print("\nğŸ“Š Plotting confusion matrices for each fold...")
        for fold_idx, fold_metric in enumerate(cv_results['fold_metrics']):
            plt.figure(figsize=(10, 8))
            cm = confusion_matrix(fold_metric['true_labels'], fold_metric['predictions'])
            
            # ä½¿ç”¨seabornç¹ªè£½æ··æ·†çŸ©é™£ç†±åœ–
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'Confusion Matrix - Fold {fold_idx + 1}')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            
            # ä¿å­˜åœ–ç‰‡
            plt.savefig(f'./images/{model_name}_fold{fold_idx + 1}_cm.png')
            plt.close()
            
            print(f"Confusion matrix for fold {fold_idx + 1} saved as: ./images/{model_name}_fold{fold_idx + 1}_cm.png")
        
        return cv_results
    
    # ç‚ºç•¶å‰ fold å‰µå»º DataLoader
    def _create_fold_dataloaders(self, X_train_fold, X_val_fold, y_train_fold, y_val_fold, batch_size=128):
        """
        ç‚ºç•¶å‰ fold å‰µå»º DataLoader
        """
        # è½‰æ›ç‚º DataFrame æ ¼å¼
        train_fold_df = pd.DataFrame(X_train_fold, columns=self.train_df_train_csv_processed.columns)
        val_fold_df = pd.DataFrame(X_val_fold, columns=self.train_df_train_csv_processed.columns)
        
        # æ·»åŠ  target åˆ—
        train_fold_df['target'] = y_train_fold
        val_fold_df['target'] = y_val_fold
        
        # å‰µå»º TabularDataset
        train_ds_fold = TabularDataset(
            data=train_fold_df,
            task='classification',
            target=['target'],
            continuous_cols=self.numerical_features_processed,
            categorical_cols=self.embedding_features_processed
        )
        
        val_ds_fold = TabularDataset(
            data=val_fold_df,
            task='classification',
            target=['target'],
            continuous_cols=self.numerical_features_processed,
            categorical_cols=self.embedding_features_processed
        )
        
        # å‰µå»º DataLoaderï¼ˆGPU å„ªåŒ–è¨­ç½®ï¼‰
        train_dl_fold = DataLoader(
            train_ds_fold, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=4, 
            pin_memory=True, 
            persistent_workers=True,
            drop_last=True  # é¿å…æœ€å¾Œä¸€å€‹batchå¤ªå°
        )
        val_dl_fold = DataLoader(
            val_ds_fold, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=4, 
            pin_memory=True, 
            persistent_workers=True
        )
        
        return train_dl_fold, val_dl_fold
    
    # è¨“ç·´å–®å€‹ fold çš„æ¨¡å‹ï¼ˆæ”¯æŒ Early Stoppingï¼‰
    def _train_fold(self, model, train_dl, val_dl, num_epochs, use_class_weight, device, fold_num, early_stopping, patience, min_delta):
        """
        è¨“ç·´å–®å€‹ fold çš„æ¨¡å‹ï¼ˆæ”¯æŒ Early Stoppingï¼‰
        """
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        # è¨­å®šæå¤±å‡½æ•¸
        if use_class_weight:
            class_weight_tensor = self._compute_class_weights(device)
            criterion = torch.nn.CrossEntropyLoss(weight=class_weight_tensor)
        else:
            criterion = torch.nn.CrossEntropyLoss()
        
        model.to(device)
        model.train()
        
        # Early Stopping ç›¸é—œè®Šæ•¸
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        early_stopped = False
        
        # è¨“ç·´å¾ªç’°
        progress_bar = tqdm(range(num_epochs), leave=False, desc=f"Fold {fold_num}")
        for epoch in progress_bar:
            # === è¨“ç·´éšæ®µ ===
            model.train()
            total_train_loss = 0
            for batch in train_dl:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                
                optimizer.zero_grad()
                outputs = model({'continuous': numerical, 'categorical': categorical})
                logits = outputs['logits']
                loss = criterion(logits, target)
                loss.backward()
                optimizer.step()
                total_train_loss += loss.item()
            
            avg_train_loss = total_train_loss / len(train_dl)
            
            # === é©—è­‰éšæ®µ ===
            model.eval()
            total_val_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch in val_dl:
                    numerical = batch['continuous'].to(device)
                    categorical = batch['categorical'].to(device)
                    target = batch['target'].to(device).squeeze()
                    
                    outputs = model({'continuous': numerical, 'categorical': categorical})
                    logits = outputs['logits']
                    loss = criterion(logits, target)
                    
                    total_val_loss += loss.item()
                    _, predicted = torch.max(logits, 1)
                    total += target.size(0)
                    correct += (predicted == target).sum().item()
            
            avg_val_loss = total_val_loss / len(val_dl)
            val_accuracy = correct / total
            
            # === Early Stopping æª¢æŸ¥ ===
            if early_stopping:
                # æª¢æŸ¥æ˜¯å¦æœ‰æ”¹å–„
                if avg_val_loss < best_val_loss - min_delta:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    model_to_save = model.module if hasattr(model, 'module') else model
                    best_model_state = model_to_save.state_dict().copy()
                else:
                    patience_counter += 1
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦ early stop
                if patience_counter >= patience:
                    early_stopped = True
                    print(f"\nâ¹ï¸ Early stopping triggered at epoch {epoch + 1}")
                    print(f"Best validation loss: {best_val_loss:.4f}")
                    break
            else:
                # ä¸ä½¿ç”¨ early stopping æ™‚ï¼Œç¸½æ˜¯ä¿å­˜æœ€æ–°çš„æ¨¡å‹
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    model_to_save = model.module if hasattr(model, 'module') else model
                    best_model_state = model_to_save.state_dict().copy()
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.set_description(
                f"Fold {fold_num} - Epoch {epoch+1}/{num_epochs} | "
                f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | "
                f"Val Acc: {val_accuracy:.4f} | Patience: {patience_counter}/{patience}"
            )
        
        # æ¢å¾©æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # æœ€çµ‚é©—è­‰
        final_val_accuracy, final_val_loss = self._validate_fold(model, val_dl, criterion, device)
        
        # è¼¸å‡ºè¨“ç·´çµæœ
        if early_stopped:
            print(f"âœ… Fold {fold_num} completed with early stopping after {epoch + 1} epochs")
        else:
            print(f"âœ… Fold {fold_num} completed full training ({num_epochs} epochs)")
        
        print(f"Final validation - Accuracy: {final_val_accuracy:.4f}, Loss: {final_val_loss:.4f}")
        
        # è¿”å›æ™‚ç¢ºä¿è¿”å›æ­£ç¢ºçš„æ¨¡å‹ç‹€æ…‹
        if best_model_state is not None:
            return final_val_accuracy, final_val_loss, best_model_state
        else:
            model_to_save = model.module if hasattr(model, 'module') else model
            return final_val_accuracy, final_val_loss, model_to_save.state_dict().copy()
    
    # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹
    def _validate_fold(self, model, val_dl, criterion, device):
        """
        åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹
        """
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in val_dl:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                
                outputs = model({'continuous': numerical, 'categorical': categorical})
                logits = outputs['logits']
                loss = criterion(logits, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        accuracy = correct / total
        avg_loss = total_loss / len(val_dl)
        
        return accuracy, avg_loss
    
    # è©³ç´°è©•ä¼°å–®å€‹ fold çš„æ€§èƒ½
    def _evaluate_fold(self, model, val_dl, device):
        """
        è©³ç´°è©•ä¼°å–®å€‹ fold çš„æ€§èƒ½
        """
        model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in val_dl:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                
                outputs = model({'continuous': numerical, 'categorical': categorical})
                logits = outputs['logits']
                _, preds = torch.max(logits, 1)
                
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(target.cpu().numpy())
        
        # è¨ˆç®—è©³ç´°æŒ‡æ¨™
        
        accuracy = accuracy_score(true_labels, predictions)
        precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)
        recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)
        f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'predictions': predictions,
            'true_labels': true_labels
        }

    # è©•ä¼°è¨“ç·´é›†ä¸¦ç¹ªè£½ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£
    def evaluate_train_set(self, model_name:str):
        """
        è©•ä¼°è¨“ç·´é›†ä¸¦ç¹ªè£½ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£
        """
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        self.model.eval()
        predictions = []
        probabilities = []
        true_labels = []
        
        with torch.no_grad():
            for batch in self.dl_train:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                
                outputs = self.model({'continuous': numerical, 'categorical': categorical})
                logits = outputs['logits']
                
                # ç²å–é æ¸¬æ¦‚ç‡
                probs = torch.softmax(logits, dim=1)
                
                # ç²å–é æ¸¬é¡åˆ¥
                _, preds = torch.max(logits, dim=1)
                
                predictions.extend(preds.cpu().numpy())
                probabilities.extend(probs.cpu().numpy())
                true_labels.extend(target.cpu().numpy())
        
        # è½‰æ›ç‚ºnumpy arrays
        predictions = np.array(predictions)
        probabilities = np.array(probabilities)
        true_labels = np.array(true_labels)
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = accuracy_score(true_labels, predictions)
        print(f'Training Set Accuracy: {accuracy:.4f}')
        
        # è¨ˆç®—æ··æ·†çŸ©é™£
        cm = confusion_matrix(true_labels, predictions)
        print('\nTraining Set Confusion Matrix:')
        print(cm)
        
        # ç¹ªè£½æ··æ·†çŸ©é™£å’ŒROCæ›²ç·š
        self._plot_train_metrics(true_labels, predictions, probabilities, cm, model_name=model_name)
        
        return accuracy, cm

    # ç¹ªè£½è¨“ç·´é›†çš„ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£
    def _plot_train_metrics(self, true_labels, predictions, probabilities, cm, model_name="model"):
        """
        ç¹ªè£½è¨“ç·´é›†çš„ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£
        
        Args:
            true_labels: çœŸå¯¦æ¨™ç±¤
            predictions: é æ¸¬æ¨™ç±¤
            probabilities: é æ¸¬æ¦‚ç‡
            cm: æ··æ·†çŸ©é™£
            model_name: æ¨¡å‹åç¨±ï¼Œç”¨æ–¼ä¿å­˜æ–‡ä»¶å
        """
        try:
            # è¨­å®šéäº’å‹•å¼å¾Œç«¯ï¼ˆé©ç”¨æ–¼æœå‹™å™¨ç’°å¢ƒï¼‰
            matplotlib.use('Agg')
            
            # è¨­å®šmatplotlibä¸­æ–‡å­—é«”
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            
            # å‰µå»ºå­åœ–
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            # === 1. ç¹ªè£½æ··æ·†çŸ©é™£ ===
            if len(np.unique(true_labels)) == 2:
                class_labels = ['Not Readmitted', 'Readmitted']
            else:
                class_labels = [f'Class {label}' for label in np.unique(true_labels)]
            
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                       xticklabels=class_labels, 
                       yticklabels=class_labels,
                       ax=axes[0])
            axes[0].set_title("Training Set - Confusion Matrix", fontsize=14, fontweight='bold')
            axes[0].set_xlabel("Predicted", fontsize=12)
            axes[0].set_ylabel("Actual", fontsize=12)
            
            # === 2. ç¹ªè£½ROCæ›²ç·š ===
            if len(np.unique(true_labels)) == 2:  # äºŒå…ƒåˆ†é¡
                # ä½¿ç”¨æ­£é¡åˆ¥ï¼ˆclass 1ï¼‰çš„æ¦‚ç‡
                y_prob_positive = probabilities[:, 1]
                
                # è¨ˆç®—ROCæ›²ç·š
                fpr, tpr, thresholds = roc_curve(true_labels, y_prob_positive)
                roc_auc = auc(fpr, tpr)
                
                # ç¹ªè£½ROCæ›²ç·š
                axes[1].plot(fpr, tpr, color='darkorange', lw=3, 
                           label=f'ROC curve (AUC = {roc_auc:.4f})')
                axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                           label='Random Classifier', alpha=0.7)
                axes[1].set_xlim([0.0, 1.0])
                axes[1].set_ylim([0.0, 1.05])
                axes[1].set_xlabel('False Positive Rate', fontsize=12)
                axes[1].set_ylabel('True Positive Rate', fontsize=12)
                axes[1].set_title('Training Set - ROC Curve', fontsize=14, fontweight='bold')
                axes[1].legend(loc="lower right")
                axes[1].grid(True, alpha=0.3)
                
                print(f"Training Set ROC AUC: {roc_auc:.4f}")
            else:
                axes[1].text(0.5, 0.5, 'ROC curve only available\nfor binary classification', 
                           ha='center', va='center', transform=axes[1].transAxes, fontsize=12)
                axes[1].set_title('Training Set - ROC Curve (N/A)', fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            
            # ä¿å­˜åœ–ç‰‡ï¼Œä½¿ç”¨model_nameå‘½å
            filename = f'{model_name}_training_metrics.png'
            plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"âœ… è¨“ç·´é›†è©•ä¼°åœ–è¡¨å·²ä¿å­˜ç‚º '{filename}'")
            
            # å˜—è©¦é¡¯ç¤ºåœ–ç‰‡ï¼ˆå¦‚æœåœ¨æ”¯æ´çš„ç’°å¢ƒä¸­ï¼‰
            try:
                plt.show()
            except:
                print("ğŸ“Š åœ–è¡¨å·²ç”Ÿæˆä¸¦ä¿å­˜ï¼Œä½†ç„¡æ³•åœ¨ç•¶å‰ç’°å¢ƒä¸­é¡¯ç¤º")
            
            # é—œé–‰åœ–å½¢ä»¥é‡‹æ”¾è¨˜æ†¶é«”
            plt.close(fig)
            
        except ImportError as e:
            print(f"âš ï¸ Warning: ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
            print("è«‹å®‰è£å¿…è¦å¥—ä»¶: pip install seaborn scikit-learn matplotlib")
        except Exception as e:
            print(f"ç„¡æ³•å‰µå»ºè¨“ç·´é›†è©•ä¼°åœ–è¡¨: {e}")
            traceback.print_exc()

    # è¨ˆç®— class weights ç”¨æ–¼å¹³è¡¡è¨“ç·´
    def _compute_class_weights(self, device):
        """
        è¨ˆç®— class weights ç”¨æ–¼å¹³è¡¡è¨“ç·´
        
        Args:
            device: é‹ç®—è¨­å‚™ ('cuda' æˆ– 'cpu')
            
        Returns:
            torch.Tensor: class weights tensor
        """
        
        # ä½¿ç”¨ç•¶å‰ï¼ˆç¶“éæ¡æ¨£å¾Œï¼‰çš„ y_train ä¾†è¨ˆç®— class weights
        y_for_weights = self.train_target_df.values
        print("Using current (post-sampling) data distribution for class weight computation")
        
        # ç²å–å”¯ä¸€é¡åˆ¥
        unique_classes = np.unique(y_for_weights)
        
        # è¨ˆç®— class weights
        class_weights = compute_class_weight(
            class_weight='balanced', 
            classes=unique_classes, 
            y=y_for_weights
        )
        
        # è½‰æ›ç‚º tensor
        class_weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)
        
        # è¼¸å‡º class weight è³‡è¨Š
        print(f"Current class distribution: {dict(zip(*np.unique(y_for_weights, return_counts=True)))}")
        for i, (class_label, weight) in enumerate(zip(unique_classes, class_weights)):
            print(f"Class {class_label}: weight = {weight:.4f}")
        
        # å¦‚æœæœ‰åŸå§‹è³‡æ–™åˆ†å¸ƒï¼Œä¹Ÿé¡¯ç¤ºå°æ¯”
        if hasattr(self, 'original_y_train'):
            original_distribution = dict(zip(*np.unique(self.original_y_train, return_counts=True)))
            print(f"Original class distribution (for reference): {original_distribution}")
        
        return class_weight_tensor

    # ç²å– class weights è³‡è¨Šï¼Œç”¨æ–¼èª¿è©¦
    def get_class_weights_info(self):
        """
        ç²å– class weights è³‡è¨Šï¼Œç”¨æ–¼èª¿è©¦
        
        Returns:
            dict: åŒ…å« class weights å’Œç›¸é—œè³‡è¨Šçš„å­—å…¸
        """        
        # ä½¿ç”¨ç•¶å‰ï¼ˆç¶“éæ¡æ¨£å¾Œï¼‰çš„è³‡æ–™åˆ†å¸ƒ
        if hasattr(self, 'train_target_df') and self.train_target_df is not None:
            y_for_weights = self.train_target_df.values
            print("Using current (post-sampling) data distribution for class weight computation")
        else:
            print("âš ï¸ Warning: train_target_df not found. Run preprocess() first.")
            return None
        
        unique_classes = np.unique(y_for_weights)
        class_weights = compute_class_weight(
            class_weight='balanced', 
            classes=unique_classes, 
            y=y_for_weights
        )
        
        unique, counts = np.unique(y_for_weights, return_counts=True)
        current_class_distribution = dict(zip(unique, counts))
        
        result = {
            'current_class_distribution': current_class_distribution,
            'class_weights': dict(zip(unique_classes, class_weights)),
            'imbalance_ratio': max(counts) / min(counts) if min(counts) > 0 else float('inf')
        }
        
        # å¦‚æœæœ‰åŸå§‹è³‡æ–™åˆ†å¸ƒï¼Œä¹Ÿæ·»åŠ åˆ°çµæœä¸­
        if hasattr(self, 'original_y_train'):
            original_unique, original_counts = np.unique(self.original_y_train, return_counts=True)
            result['original_class_distribution'] = dict(zip(original_unique, original_counts))
            result['original_imbalance_ratio'] = max(original_counts) / min(original_counts)
        
        return result

    # è©•ä¼°æ¨¡å‹
    def evaluate_model(self, model_name: str):
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # å˜—è©¦è¼‰å…¥æ¨¡å‹æª”æ¡ˆ
        try:
            model_state = torch.load(f"./models/{model_name}")
            print(f"âœ… æ¨¡å‹æª”æ¡ˆè¼‰å…¥æˆåŠŸ: ./models/{model_name}")
        except FileNotFoundError:
            print(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: ./models/{model_name}")
            return None, None
        
        # æª¢æŸ¥æ¨¡å‹æ¶æ§‹æ˜¯å¦åŒ¹é…
        try:
            # å…ˆå˜—è©¦ç”¨ç•¶å‰é…ç½®è¼‰å…¥
            self.model = FTTransformerModel(config=self.config)
            self.model.load_state_dict(model_state)
            print("âœ… æ¨¡å‹æ¶æ§‹åŒ¹é…ï¼Œç›´æ¥è¼‰å…¥æˆåŠŸ")
        except RuntimeError as e:
            if "Unexpected key(s) in state_dict" in str(e):
                print("âš ï¸ æ¨¡å‹æ¶æ§‹ä¸åŒ¹é…ï¼Œå˜—è©¦è‡ªå‹•èª¿æ•´...")
                
                # åˆ†æä¿å­˜çš„æ¨¡å‹æœ‰å¤šå°‘å€‹attention blocks
                attention_block_keys = [key for key in model_state.keys() if "mha_block_" in key]
                if attention_block_keys:
                    # æå–æœ€å¤§çš„blockç·¨è™Ÿ
                    max_block_num = max([int(key.split("mha_block_")[1].split(".")[0]) for key in attention_block_keys])
                    inferred_num_blocks = max_block_num + 1  # blockç·¨è™Ÿå¾0é–‹å§‹
                    
                    print(f"ğŸ” æª¢æ¸¬åˆ°ä¿å­˜çš„æ¨¡å‹æœ‰ {inferred_num_blocks} å€‹attention blocks")
                    print(f"ğŸ“ ç•¶å‰é…ç½®çš„attention blocks: {self.num_attn_blocks}")
                    
                    # æ›´æ–°å¯¦ä¾‹è®Šé‡
                    self.num_attn_blocks = inferred_num_blocks
                    
                    # é‡æ–°å‰µå»ºé…ç½®
                    adjusted_config = FTTransformerConfig(
                        task="classification",
                        num_attn_blocks=self.num_attn_blocks,
                    )
                    adjusted_config = adjusted_config.merge_dataset_config(self.ds_train)
                    
                    # ç”¨èª¿æ•´å¾Œçš„é…ç½®å‰µå»ºæ¨¡å‹
                    self.model = FTTransformerModel(config=adjusted_config)
                    self.model.load_state_dict(model_state)
                    print(f"âœ… æ¨¡å‹æ¶æ§‹å·²èª¿æ•´ç‚º {self.num_attn_blocks} å€‹attention blocksä¸¦è¼‰å…¥æˆåŠŸ")
                    
                    # æ›´æ–°æ¨¡å‹é…ç½®
                    self.config = adjusted_config
                else:
                    print("âŒ ç„¡æ³•è‡ªå‹•åˆ¤æ–·æ¨¡å‹æ¶æ§‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ")
                    raise e
            else:
                raise e
        
        self.model.to(device=device)        
        self.model.eval()
        
        predictions = []
        true_labels = []
        with torch.no_grad():
            for batch in self.dl_test:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                target = batch['target'].to(device).squeeze()
                outputs = self.model({'continuous': numerical, 'categorical': categorical})
                _, preds = torch.max(outputs['logits'], dim=1)
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(target.cpu().numpy())
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = accuracy_score(true_labels, predictions)
        print(f'Test Accuracy: {accuracy:.4f}')
        
        # é¡¯ç¤ºåˆ†é¡å ±å‘Š
        print('\nClassification Report:')
        print(classification_report(true_labels, predictions))
        
        # é¡¯ç¤ºæ··æ·†çŸ©é™£
        print('\nConfusion Matrix:')
        cm = confusion_matrix(true_labels, predictions)
        print(cm)
        
        # å‰µå»ºæ›´è©³ç´°çš„æ··æ·†çŸ©é™£é¡¯ç¤º
        print('\nDetailed Confusion Matrix:')
        unique_labels = sorted(list(set(true_labels + predictions)))
        
        # æ‰“å°è¡¨é ­
        print('Predicted:', end='')
        for label in unique_labels:
            print(f'{label:>8}', end='')
        print()
        
        # æ‰“å°æ¯ä¸€è¡Œ
        for i, true_label in enumerate(unique_labels):
            print(f'Actual {true_label}:', end='')
            for j, pred_label in enumerate(unique_labels):
                print(f'{cm[i,j]:>8}', end='')
            print()
        
        # å¯é¸ï¼šå‰µå»ºè¦–è¦ºåŒ–çš„æ··æ·†çŸ©é™£
        self._plot_confusion_matrix(cm, unique_labels)
        
        return accuracy, cm

    def _plot_confusion_matrix(self, cm, labels):
        """
        ç¹ªè£½æ··æ·†çŸ©é™£çš„è¦–è¦ºåŒ–åœ–è¡¨
        """
        try:            
            # è¨­å®šæ¨™ç±¤æ˜ å°„
            if len(labels) == 2:
                class_labels = ['Not Readmitted', 'Readmitted']
            else:
                class_labels = [f'Class {label}' for label in labels]
            
            plt.figure(figsize=(6, 4))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                       xticklabels=class_labels, 
                       yticklabels=class_labels)
            plt.title("Confusion Matrix")
            plt.xlabel("Predicted")
            plt.ylabel("Actual")
            plt.tight_layout()
            plt.show()
            
            print("æ··æ·†çŸ©é™£åœ–è¡¨å·²ä¿å­˜ç‚º 'confusion_matrix.png'")
            
        except ImportError:
            print("Seaborn æœªå®‰è£ï¼Œç„¡æ³•é¡¯ç¤ºè¦–è¦ºåŒ–æ··æ·†çŸ©é™£")
            print("è«‹å®‰è£ seaborn: pip install seaborn")
        except Exception as e:
            print(f"ç„¡æ³•å‰µå»ºè¦–è¦ºåŒ–æ··æ·†çŸ©é™£: {e}")

    # æ¸¬è©¦ test_df.csv æ¨¡å‹è©•ä¼°
    def test_csv_model_eval(self):
        # ä½¿ç”¨è¨“ç·´æ™‚çš„ LabelEncoder ä¾†ç·¨ç¢¼é¡åˆ¥ç‰¹å¾µ
        if hasattr(self, 'label_encoders'):
            for col in CATEGORICAL_FEATURES:
                # ä½¿ç”¨è¨“ç·´æ™‚ä¿å­˜çš„ LabelEncoder
                self.test_df_test_csv[col] = self.label_encoders[col].transform(self.test_df_test_csv[col])
                self.test_df_test_csv[col] = self.test_df_test_csv[col].astype('category')
        else:
            print("âš ï¸ Warning: No label encoders found. Run format_dataframe() first.")
            # å°‡é¡åˆ¥ç‰¹å¾µè½‰ç‚º category é¡å‹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            for col in CATEGORICAL_FEATURES:
                self.test_df_test_csv[col] = self.test_df_test_csv[col].astype('category')

        # ä½¿ç”¨è¨“ç·´æ™‚çš„ scaler ä¾†æ¨™æº–åŒ–æ¸¬è©¦æ•¸æ“š
        X_num_test_csv = self.test_df_test_csv[NUMERICAL_FEATURES].values.astype(np.float32)
        X_num_test_csv = self.scaler.transform(X_num_test_csv)  # ä½¿ç”¨å·²ç¶“ fit çš„ scaler
        X_cat_test_csv = self.test_df_test_csv[CATEGORICAL_FEATURES].values

        self.test_df_test_csv = pd.DataFrame({
            **{f'num_{i}': X_num_test_csv[:, i] for i in range(X_num_test_csv.shape[1])},
            **{f'cat_{i}': X_cat_test_csv[:, i] for i in range(X_cat_test_csv.shape[1])},
        })

        test_df_test_csv_processed = self.preprocessor.transform(self.test_df_test_csv)
        test_df_test_csv_processed = test_df_test_csv_processed.loc[:, ~test_df_test_csv_processed.columns.duplicated(keep='last')]

        ds_test_test_csv = TabularDataset(
            data=test_df_test_csv_processed,
            task='classification',
            continuous_cols=self.numerical_features_processed,
            categorical_cols=self.embedding_features_processed
        )

        dl_test_test_csv = DataLoader(ds_test_test_csv, batch_size=128, shuffle=False, num_workers=0, pin_memory=False)
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = FTTransformerModel(config=self.config)
        self.model.load_state_dict(torch.load("ft_transformer_model.pth"))
        self.model.to(device)

        predictions = []

        # evaluation mode
        self.model.eval()
        with torch.no_grad():
            for batch in dl_test_test_csv:
                numerical = batch['continuous'].to(device)
                categorical = batch['categorical'].to(device)
                outputs = self.model({'continuous': numerical, 'categorical': categorical})
                _, preds = torch.max(outputs['logits'], dim=1)
                predictions.extend(preds.cpu().numpy())

        submission_df = pd.DataFrame({
            "Patient_ID": range(1, len(predictions)+1),
            "readmitted": predictions
        })

        submission_df.to_csv(path_or_buf='data/submission_df.csv', index=False)

    # å–®ç¨ç¹ªè£½è¨“ç·´é›†çš„ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£ï¼ˆç„¡éœ€é‡æ–°è¨“ç·´ï¼‰
    def plot_train_roc_and_confusion_matrix(self, model_name=None):
        """
        å–®ç¨ç¹ªè£½è¨“ç·´é›†çš„ROCæ›²ç·šå’Œæ··æ·†çŸ©é™£ï¼ˆç„¡éœ€é‡æ–°è¨“ç·´ï¼‰
        
        Args:
            model_name (str, optional): å¦‚æœæä¾›ï¼Œæœƒè¼‰å…¥æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶
        """
        if model_name:
            # è¼‰å…¥æŒ‡å®šçš„æ¨¡å‹
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # å˜—è©¦è¼‰å…¥æ¨¡å‹æª”æ¡ˆ
            try:
                model_state = torch.load(f"./models/{model_name}")
                print(f"âœ… æ¨¡å‹æª”æ¡ˆè¼‰å…¥æˆåŠŸ: ./models/{model_name}")
            except FileNotFoundError:
                print(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: ./models/{model_name}")
                return None
            
            # æª¢æŸ¥æ¨¡å‹æ¶æ§‹æ˜¯å¦åŒ¹é…
            try:
                # å…ˆå˜—è©¦ç”¨ç•¶å‰é…ç½®è¼‰å…¥
                self.model = FTTransformerModel(config=self.config)
                self.model.load_state_dict(model_state)
                print("âœ… æ¨¡å‹æ¶æ§‹åŒ¹é…ï¼Œç›´æ¥è¼‰å…¥æˆåŠŸ")
            except RuntimeError as e:
                if "Unexpected key(s) in state_dict" in str(e):
                    print("âš ï¸ æ¨¡å‹æ¶æ§‹ä¸åŒ¹é…ï¼Œå˜—è©¦è‡ªå‹•èª¿æ•´...")
                    
                    # åˆ†æä¿å­˜çš„æ¨¡å‹æœ‰å¤šå°‘å€‹attention blocks
                    attention_block_keys = [key for key in model_state.keys() if "mha_block_" in key]
                    if attention_block_keys:
                        # æå–æœ€å¤§çš„blockç·¨è™Ÿ
                        max_block_num = max([int(key.split("mha_block_")[1].split(".")[0]) for key in attention_block_keys])
                        inferred_num_blocks = max_block_num + 1  # blockç·¨è™Ÿå¾0é–‹å§‹
                        
                        print(f"ğŸ” æª¢æ¸¬åˆ°ä¿å­˜çš„æ¨¡å‹æœ‰ {inferred_num_blocks} å€‹attention blocks")
                        print(f"ğŸ“ ç•¶å‰é…ç½®çš„attention blocks: {self.num_attn_blocks}")
                        
                        # æ›´æ–°å¯¦ä¾‹è®Šé‡
                        self.num_attn_blocks = inferred_num_blocks
                        
                        # é‡æ–°å‰µå»ºé…ç½®
                        adjusted_config = FTTransformerConfig(
                            task="classification",
                            num_attn_blocks=self.num_attn_blocks,
                        )
                        adjusted_config = adjusted_config.merge_dataset_config(self.ds_train)
                        
                        # ç”¨èª¿æ•´å¾Œçš„é…ç½®å‰µå»ºæ¨¡å‹
                        self.model = FTTransformerModel(config=adjusted_config)
                        self.model.load_state_dict(model_state)
                        print(f"âœ… æ¨¡å‹æ¶æ§‹å·²èª¿æ•´ç‚º {self.num_attn_blocks} å€‹attention blocksä¸¦è¼‰å…¥æˆåŠŸ")
                        
                        # æ›´æ–°æ¨¡å‹é…ç½®
                        self.config = adjusted_config
                    else:
                        print("âŒ ç„¡æ³•è‡ªå‹•åˆ¤æ–·æ¨¡å‹æ¶æ§‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ")
                        raise e
                else:
                    raise e
            
            self.model.to(device)
            print(f"Model loaded from ./models/{model_name}")
        
        if self.model is None:
            print("âš ï¸ Error: No model available. Please train a model first or provide model_name.")
            return None
        
        print("ğŸ” Evaluating training set...")
        return self.evaluate_train_set(model_name=model_name)

    def get_model_info(self):
        """
        é¡¯ç¤ºç•¶å‰æ¨¡å‹é…ç½®è³‡è¨Š
        """
        print(f"\n{'='*50}")
        print(f"ğŸ¤– æ¨¡å‹é…ç½®è³‡è¨Š")
        print(f"{'='*50}")
        print(f"Attention Blocks: {self.num_attn_blocks}")
        print(f"Task: {self.model_config.task}")
        
        if hasattr(self, 'config') and self.config is not None:
            print(f"é…ç½®å·²åˆä½µ: âœ…")
            if hasattr(self.config, 'd_out'):
                print(f"è¼¸å‡ºç¶­åº¦: {self.config.d_out}")
        else:
            print(f"é…ç½®å·²åˆä½µ: âŒ")
            
        if hasattr(self, 'model') and self.model is not None:
            print(f"æ¨¡å‹å·²åˆå§‹åŒ–: âœ…")
            # è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            print(f"ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
            print(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
        else:
            print(f"æ¨¡å‹å·²åˆå§‹åŒ–: âŒ")
        
        print(f"{'='*50}")
        
        return {
            'num_attn_blocks': self.num_attn_blocks,
            'task': self.model_config.task,
            'config_merged': hasattr(self, 'config') and self.config is not None,
            'model_initialized': hasattr(self, 'model') and self.model is not None
        }