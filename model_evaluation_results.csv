Model,Accuracy,Macro Recall,Weighted Recall,Error
ft_transformer_model,0.77,0.5296123124878239,0.77,
ft_transformer_model_tomek_under_sampling.pth,0.621,0.5074656981436643,0.621,
ft_transformer_model_smote_nc.pth,0.657,0.5065751022793687,0.657,
test_model.pth,0.217,0.4988102195875428,0.217,
ft_transformer_model_class_weight.pth,0.609,0.4865923575742396,0.609,
ft_transformer_model.pth,0.0,0.0,0.0,"評估模型 ft_transformer_model.pth 時發生錯誤: Error(s) in loading state_dict for FTTransformerModel:
	Unexpected key(s) in state_dict: ""_embedding_layer.cat_embedding_layers.3.shared_embed"", ""_embedding_layer.cat_embedding_layers.3.embed.weight"", ""_embedding_layer.cat_embedding_layers.4.shared_embed"", ""_embedding_layer.cat_embedding_layers.4.embed.weight"", ""_embedding_layer.cat_embedding_layers.5.shared_embed"", ""_embedding_layer.cat_embedding_layers.5.embed.weight"". 
	size mismatch for _backbone.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.cat_embedding_bias: copying a param with shape torch.Size([6, 32]) from checkpoint, the shape in current model is torch.Size([3, 32]).
	size mismatch for _embedding_layer.cont_embedding_bias: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.cont_embedding_layer.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4])."
ft_transformer_model_borderline.pth,0.0,0.0,0.0,"評估模型 ft_transformer_model_borderline.pth 時發生錯誤: Error(s) in loading state_dict for FTTransformerModel:
	Unexpected key(s) in state_dict: ""_embedding_layer.cat_embedding_layers.3.shared_embed"", ""_embedding_layer.cat_embedding_layers.3.embed.weight"", ""_embedding_layer.cat_embedding_layers.4.shared_embed"", ""_embedding_layer.cat_embedding_layers.4.embed.weight"", ""_embedding_layer.cat_embedding_layers.5.shared_embed"", ""_embedding_layer.cat_embedding_layers.5.embed.weight"". 
	size mismatch for _backbone.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.cat_embedding_bias: copying a param with shape torch.Size([6, 32]) from checkpoint, the shape in current model is torch.Size([3, 32]).
	size mismatch for _embedding_layer.cont_embedding_bias: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.cont_embedding_layer.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4])."
ft_transformer_model_enn_under_sampling.pth,0.0,0.0,0.0,"評估模型 ft_transformer_model_enn_under_sampling.pth 時發生錯誤: Error(s) in loading state_dict for FTTransformerModel:
	Unexpected key(s) in state_dict: ""_embedding_layer.cat_embedding_layers.3.shared_embed"", ""_embedding_layer.cat_embedding_layers.3.embed.weight"", ""_embedding_layer.cat_embedding_layers.4.shared_embed"", ""_embedding_layer.cat_embedding_layers.4.embed.weight"", ""_embedding_layer.cat_embedding_layers.5.shared_embed"", ""_embedding_layer.cat_embedding_layers.5.embed.weight"". 
	size mismatch for _backbone.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.cat_embedding_bias: copying a param with shape torch.Size([6, 32]) from checkpoint, the shape in current model is torch.Size([3, 32]).
	size mismatch for _embedding_layer.cont_embedding_bias: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.cont_embedding_layer.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4])."
ft_transformer_model_random_under_sampling.pth,0.0,0.0,0.0,"評估模型 ft_transformer_model_random_under_sampling.pth 時發生錯誤: Error(s) in loading state_dict for FTTransformerModel:
	Unexpected key(s) in state_dict: ""_embedding_layer.cat_embedding_layers.3.shared_embed"", ""_embedding_layer.cat_embedding_layers.3.embed.weight"", ""_embedding_layer.cat_embedding_layers.4.shared_embed"", ""_embedding_layer.cat_embedding_layers.4.embed.weight"", ""_embedding_layer.cat_embedding_layers.5.shared_embed"", ""_embedding_layer.cat_embedding_layers.5.embed.weight"". 
	size mismatch for _backbone.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.cat_embedding_bias: copying a param with shape torch.Size([6, 32]) from checkpoint, the shape in current model is torch.Size([3, 32]).
	size mismatch for _embedding_layer.cont_embedding_bias: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.cont_embedding_layer.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4])."
ft_transformer_model_smote.pth,0.0,0.0,0.0,"評估模型 ft_transformer_model_smote.pth 時發生錯誤: Error(s) in loading state_dict for FTTransformerModel:
	Unexpected key(s) in state_dict: ""_embedding_layer.cat_embedding_layers.3.shared_embed"", ""_embedding_layer.cat_embedding_layers.3.embed.weight"", ""_embedding_layer.cat_embedding_layers.4.shared_embed"", ""_embedding_layer.cat_embedding_layers.4.embed.weight"", ""_embedding_layer.cat_embedding_layers.5.shared_embed"", ""_embedding_layer.cat_embedding_layers.5.embed.weight"". 
	size mismatch for _backbone.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _backbone.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.cat_embedding_bias: copying a param with shape torch.Size([6, 32]) from checkpoint, the shape in current model is torch.Size([3, 32]).
	size mismatch for _embedding_layer.cont_embedding_bias: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.cont_embedding_layer.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([4, 32]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4]).
	size mismatch for _embedding_layer.normalizing_batch_norm.bn.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4])."
