import os
import sys
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import classification_report
from typing import Dict, List, Tuple, Optional

# å°‡srcè³‡æ–™å¤¾åŠ å…¥è·¯å¾‘ï¼Œä»¥ä¾¿å°å…¥ft_transformeræ¨¡çµ„
sys.path.append('src')
from ft_transformer import CustomerFTTransformer


class ModelEvaluator:
    """
    ä¸€å€‹ç”¨æ–¼è©•ä¼°æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦æ‰¾å‡ºrecallæœ€é«˜çš„æ¨¡å‹é¡åˆ¥
    
    åŠŸèƒ½:
    1. ä½¿ç”¨ft_transformer.pyè£¡é¢çš„evaluate_modelç¨‹å¼
    2. ä½¿ç”¨modelsè³‡æ–™å¤¾è£¡é¢çš„æ‰€æœ‰model
    3. æ‰¾å‡ºrecallæœ€é«˜çš„model
    4. æä¾›è©³ç´°çš„è©•ä¼°å ±å‘Š
    """
    
    def __init__(self, models_dir: str = "models"):
        """
        åˆå§‹åŒ–ModelEvaluator
        
        Args:
            models_dir (str): æ¨¡å‹æª”æ¡ˆè³‡æ–™å¤¾è·¯å¾‘
        """
        self.models_dir = Path(models_dir)
        
        # åˆå§‹åŒ–FT-Transformerï¼ˆä½¿ç”¨é è¨­è¨­å®šä»¥é¿å…è¤‡é›œåŒ–ï¼‰
        self.ft_transformer = CustomerFTTransformer()
        
        # å­˜å„²è©•ä¼°çµæœ
        self.evaluation_results: Dict[str, Dict] = {}
        self.best_recall_model: Optional[str] = None
        self.best_recall_score: float = 0.0
        
        # é è™•ç†è³‡æ–™
        self._prepare_data()
    
    def _prepare_data(self):
        """æº–å‚™è³‡æ–™é€²è¡Œè©•ä¼°"""
        print("ğŸ”„ æ­£åœ¨æº–å‚™è³‡æ–™...")
        
        try:
            # æ ¼å¼åŒ–è³‡æ–™æ¡†
            self.ft_transformer.format_dataframe()
            
            # é è™•ç†è³‡æ–™ï¼ˆä½¿ç”¨é è¨­è¨­å®šï¼‰
            self.ft_transformer.preprocess(
                use_smote=False,
                use_under_sampling=False
            )
            
            # è¨­å®šç‰¹å¾µ
            self.ft_transformer.set_feautres_processed()
            
            # å»ºç«‹è³‡æ–™é›†
            self.ft_transformer.set_tablar_dataset()
            
            # è¨­å®šæ¨¡å‹é…ç½®
            self.ft_transformer.set_model_config()
            
            print("âœ… è³‡æ–™æº–å‚™å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ è³‡æ–™æº–å‚™å¤±æ•—: {e}")
            raise
    
    def get_model_files(self) -> List[str]:
        """
        ç²å–modelsè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
        
        Returns:
            List[str]: æ¨¡å‹æª”æ¡ˆåç¨±åˆ—è¡¨
        """
        if not self.models_dir.exists():
            raise FileNotFoundError(f"æ¨¡å‹è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.models_dir}")
        
        # å°‹æ‰¾æ‰€æœ‰.pthæª”æ¡ˆå’Œç„¡å‰¯æª”åçš„æ¨¡å‹æª”æ¡ˆ
        model_files = []
        for file_path in self.models_dir.iterdir():
            if file_path.is_file():
                if (file_path.suffix == '.pth' or 
                    ('model' in file_path.name.lower() and file_path.suffix == '')):
                    model_files.append(file_path.name)
        
        if not model_files:
            raise ValueError(f"åœ¨ {self.models_dir} ä¸­æ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ")
        
        return sorted(model_files)
    
    def evaluate_single_model(self, model_name: str) -> Dict:
        """
        è©•ä¼°å–®ä¸€æ¨¡å‹ä¸¦è¨ˆç®—recallå€¼
        
        Args:
            model_name (str): æ¨¡å‹æª”æ¡ˆåç¨±
            
        Returns:
            Dict: åŒ…å«è©•ä¼°çµæœçš„å­—å…¸
        """
        print(f"\nğŸ” æ­£åœ¨è©•ä¼°æ¨¡å‹: {model_name}")
        
        try:
            # ä½¿ç”¨ft_transformerçš„evaluate_modelæ–¹æ³•
            accuracy, confusion_matrix = self.ft_transformer.evaluate_model(model_name)
            
            # ç²å–é æ¸¬çµæœä»¥è¨ˆç®—è©³ç´°æŒ‡æ¨™
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            predictions = []
            true_labels = []
            
            self.ft_transformer.model.eval()
            with torch.no_grad():
                for batch in self.ft_transformer.dl_test:
                    numerical = batch['continuous'].to(device)
                    categorical = batch['categorical'].to(device)
                    target = batch['target'].to(device).squeeze()
                    outputs = self.ft_transformer.model({'continuous': numerical, 'categorical': categorical})
                    _, preds = torch.max(outputs['logits'], dim=1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(target.cpu().numpy())
            
            # è¨ˆç®—åˆ†é¡å ±å‘Š
            class_report = classification_report(true_labels, predictions, output_dict=True)
            
            # æå–recallå€¼
            macro_recall = class_report['macro avg']['recall']
            weighted_recall = class_report['weighted avg']['recall']
            
            # è¨ˆç®—å„é¡åˆ¥çš„recall
            individual_recalls = {}
            for key, metrics in class_report.items():
                if key not in ['accuracy', 'macro avg', 'weighted avg']:
                    individual_recalls[f'class_{key}'] = metrics.get('recall', 0.0)
            
            result = {
                'model_name': model_name,
                'accuracy': accuracy,
                'macro_recall': macro_recall,
                'weighted_recall': weighted_recall,
                'individual_recalls': individual_recalls,
                'confusion_matrix': confusion_matrix.tolist(),
                'classification_report': class_report
            }
            
            print(f"âœ… æ¨¡å‹ {model_name} è©•ä¼°å®Œæˆ")
            print(f"   æº–ç¢ºç‡: {accuracy:.4f}")
            print(f"   Macro Recall: {macro_recall:.4f}")
            print(f"   Weighted Recall: {weighted_recall:.4f}")
            
            return result
            
        except Exception as e:
            error_msg = f"è©•ä¼°æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                'model_name': model_name,
                'error': error_msg,
                'accuracy': 0.0,
                'macro_recall': 0.0,
                'weighted_recall': 0.0
            }
    
    def evaluate_all_models(self, max_models: Optional[int] = None) -> Dict[str, Dict]:
        """
        è©•ä¼°æ‰€æœ‰æ¨¡å‹
        
        Args:
            max_models (Optional[int]): æœ€å¤§è©•ä¼°æ¨¡å‹æ•¸é‡ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
            
        Returns:
            Dict[str, Dict]: åŒ…å«æ‰€æœ‰æ¨¡å‹è©•ä¼°çµæœçš„å­—å…¸
        """
        print(f"\nğŸš€ é–‹å§‹è©•ä¼°æ‰€æœ‰æ¨¡å‹...")
        
        model_files = self.get_model_files()
        if max_models:
            model_files = model_files[:max_models]
            print(f"é™åˆ¶è©•ä¼° {max_models} å€‹æ¨¡å‹é€²è¡Œæ¸¬è©¦")
        
        print(f"æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹æª”æ¡ˆ:")
        for i, model_file in enumerate(model_files, 1):
            print(f"  {i}. {model_file}")
        
        # é‡ç½®çµæœ
        self.evaluation_results = {}
        self.best_recall_model = None
        self.best_recall_score = 0.0
        
        # è©•ä¼°æ¯å€‹æ¨¡å‹
        for i, model_file in enumerate(model_files, 1):
            print(f"\nğŸ“ é€²åº¦: {i}/{len(model_files)}")
            result = self.evaluate_single_model(model_file)
            self.evaluation_results[model_file] = result
            
            # æ›´æ–°æœ€ä½³recallæ¨¡å‹
            current_recall = result.get('macro_recall', 0.0)
            if current_recall > self.best_recall_score:
                self.best_recall_score = current_recall
                self.best_recall_model = model_file
                print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹: {model_file} (Macro Recall: {current_recall:.4f})")
        
        print(f"\nğŸ‰ è©•ä¼°å®Œæˆï¼")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {self.best_recall_model}")
        print(f"ğŸ¯ æœ€ä½³ Macro Recall: {self.best_recall_score:.4f}")
        
        return self.evaluation_results
    
    def get_best_model(self) -> Tuple[Optional[str], float]:
        """
        ç²å–recallæœ€é«˜çš„æ¨¡å‹
        
        Returns:
            Tuple[Optional[str], float]: (æœ€ä½³æ¨¡å‹åç¨±, æœ€ä½³recallåˆ†æ•¸)
        """
        return self.best_recall_model, self.best_recall_score
    
    def get_results_summary(self) -> pd.DataFrame:
        """
        ç²å–æ‰€æœ‰æ¨¡å‹çš„è©•ä¼°çµæœæ‘˜è¦
        
        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰æ¨¡å‹è©•ä¼°çµæœçš„DataFrame
        """
        if not self.evaluation_results:
            return pd.DataFrame()
        
        summary_data = []
        for model_name, result in self.evaluation_results.items():
            summary_data.append({
                'Model': model_name,
                'Accuracy': result.get('accuracy', 0.0),
                'Macro_Recall': result.get('macro_recall', 0.0),
                'Weighted_Recall': result.get('weighted_recall', 0.0),
                'Class_0_Recall': result.get('individual_recalls', {}).get('class_0', 0.0),
                'Class_1_Recall': result.get('individual_recalls', {}).get('class_1', 0.0),
                'Error': result.get('error', '')
            })
        
        df = pd.DataFrame(summary_data)
        # æŒ‰ç…§Macro Recallé™åºæ’åˆ—
        df = df.sort_values('Macro_Recall', ascending=False).reset_index(drop=True)
        
        return df
    
    def save_results(self, output_file: str = 'model_evaluation_results.csv'):
        """
        å°‡è©•ä¼°çµæœä¿å­˜åˆ°CSVæª”æ¡ˆ
        
        Args:
            output_file (str): è¼¸å‡ºæª”æ¡ˆåç¨±
        """
        summary_df = self.get_results_summary()
        if not summary_df.empty:
            summary_df.to_csv(output_file, index=False)
            print(f"ğŸ“Š è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("âš ï¸ æ²’æœ‰è©•ä¼°çµæœå¯ä¿å­˜")
    
    def print_summary(self):
        """å°å‡ºè©•ä¼°çµæœæ‘˜è¦"""
        if not self.evaluation_results:
            print("âš ï¸ å°šæœªé€²è¡Œæ¨¡å‹è©•ä¼°")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š æ¨¡å‹è©•ä¼°çµæœæ‘˜è¦")
        print("="*80)
        
        summary_df = self.get_results_summary()
        print(summary_df.to_string(index=False, float_format='{:.4f}'.format))
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰Macro Recallæ’åºï¼‰:")
        print(f"   æ¨¡å‹åç¨±: {self.best_recall_model}")
        print(f"   Macro Recall: {self.best_recall_score:.4f}")
        
        # é¡¯ç¤ºå„é¡åˆ¥è©³ç´°recall
        if self.best_recall_model and self.best_recall_model in self.evaluation_results:
            best_result = self.evaluation_results[self.best_recall_model]
            if 'individual_recalls' in best_result:
                print(f"   å„é¡åˆ¥Recall:")
                for class_name, recall in best_result['individual_recalls'].items():
                    print(f"     {class_name}: {recall:.4f}")


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ å•Ÿå‹•æ¨¡å‹è©•ä¼°å™¨...")
    
    try:
        # å‰µå»ºè©•ä¼°å™¨
        evaluator = ModelEvaluator(models_dir="models")
        
        # è©•ä¼°æ‰€æœ‰æ¨¡å‹
        results = evaluator.evaluate_all_models()
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        evaluator.print_summary()
        
        # ä¿å­˜çµæœ
        evaluator.save_results('model_evaluation_results.csv')
        
        print("\nâœ… è©•ä¼°å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 