import os
import sys
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import classification_report
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯

# å°‡srcè³‡æ–™å¤¾åŠ å…¥è·¯å¾‘ï¼Œä»¥ä¾¿å°å…¥ft_transformeræ¨¡çµ„
sys.path.append('src')
from ft_transformer import CustomerFTTransformer


class ModelEvaluator:
    """
    ä¸€å€‹ç”¨æ–¼è©•ä¼°æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦æ‰¾å‡ºrecallæœ€é«˜çš„æ¨¡å‹é¡åˆ¥
    
    This class handles:
    1. ä½¿ç”¨ft_transformer.pyè£¡é¢çš„evaluate_modelç¨‹å¼
    2. ä½¿ç”¨modelsè³‡æ–™å¤¾è£¡é¢çš„æ‰€æœ‰model
    3. æ‰¾å‡ºrecallæœ€é«˜çš„model
    4. ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆè¦–è¦ºåŒ–åœ–ç‰‡ä¸¦å­˜åˆ°imagesè³‡æ–™å¤¾
    """
    
    def __init__(self, models_dir: str = "models", images_dir: str = "images", 
                 use_smote: bool = False, use_under_sampling: bool = False, 
                 smote_method: str = 'smotenc', under_sampling_method: str = 'tomek'):
        """
        åˆå§‹åŒ–ModelEvaluator
        
        Args:
            models_dir (str): æ¨¡å‹æª”æ¡ˆè³‡æ–™å¤¾è·¯å¾‘
            images_dir (str): åœ–ç‰‡è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘
            use_smote (bool): æ˜¯å¦ä½¿ç”¨SMOTEé€²è¡Œè³‡æ–™å¹³è¡¡
            use_under_sampling (bool): æ˜¯å¦ä½¿ç”¨under sampling
            smote_method (str): SMOTEæ–¹æ³•
            under_sampling_method (str): Under samplingæ–¹æ³•
        """
        self.models_dir = Path(models_dir)
        self.images_dir = Path(images_dir)
        self.use_smote = use_smote
        self.use_under_sampling = use_under_sampling
        self.smote_method = smote_method
        self.under_sampling_method = under_sampling_method
        
        # å‰µå»ºimagesè³‡æ–™å¤¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.images_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ åœ–ç‰‡å°‡ä¿å­˜åˆ°: {self.images_dir}")
        
        # åˆå§‹åŒ–FT-Transformer
        self.ft_transformer = CustomerFTTransformer()
        
        # å­˜å„²è©•ä¼°çµæœ
        self.evaluation_results: Dict[str, Dict] = {}
        self.best_recall_model: Optional[str] = None
        self.best_recall_score: float = 0.0
        
        # é è™•ç†è³‡æ–™
        self._prepare_data()
    
    def _prepare_data(self):
        """æº–å‚™è³‡æ–™é€²è¡Œè©•ä¼°"""
        print("ğŸ”„ æ­£åœ¨æº–å‚™è³‡æ–™...")
        
        # æ ¼å¼åŒ–è³‡æ–™æ¡†
        self.ft_transformer.format_dataframe()
        
        # é è™•ç†è³‡æ–™
        self.ft_transformer.preprocess(
            use_smote=self.use_smote,
            use_under_sampling=self.use_under_sampling,
            smote_method=self.smote_method,
            under_sampling_method=self.under_sampling_method
        )
        
        # è¨­å®šç‰¹å¾µï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±ï¼‰
        self.ft_transformer.set_feautres_processed()
        
        # å»ºç«‹è³‡æ–™é›†ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±ï¼‰
        self.ft_transformer.set_tablar_dataset()
        
        # è¨­å®šæ¨¡å‹é…ç½®
        self.ft_transformer.set_model_config()
        
        print("âœ… è³‡æ–™æº–å‚™å®Œæˆ")
    
    def _plot_confusion_matrix(self, cm, labels, model_name: str):
        """
        ç‚ºç‰¹å®šæ¨¡å‹ç¹ªè£½æ··æ·†çŸ©é™£çš„è¦–è¦ºåŒ–åœ–è¡¨ä¸¦ä¿å­˜
        
        Args:
            cm: æ··æ·†çŸ©é™£
            labels: æ¨™ç±¤åˆ—è¡¨
            model_name: æ¨¡å‹åç¨±
        """
        try:
            # è¨­å®šæ¨™ç±¤æ˜ å°„
            if len(labels) == 2:
                class_labels = ['Not Readmitted', 'Readmitted']
            else:
                class_labels = [f'Class {label}' for label in labels]
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                       xticklabels=class_labels, 
                       yticklabels=class_labels)
            plt.title(f"Confusion Matrix - {model_name}")
            plt.xlabel("Predicted")
            plt.ylabel("Actual")
            plt.tight_layout()
            
            # æ¸…ç†æ¨¡å‹åç¨±ç”¨æ–¼æª”æ¡ˆåç¨±
            safe_model_name = model_name.replace('.pth', '').replace('/', '_').replace('\\', '_')
            confusion_matrix_file = self.images_dir / f"confusion_matrix_{safe_model_name}.png"
            
            plt.savefig(confusion_matrix_file, dpi=300, bbox_inches='tight')
            print(f"  ğŸ–¼ï¸ æ··æ·†çŸ©é™£åœ–è¡¨å·²ä¿å­˜: {confusion_matrix_file}")
            plt.close()
            
        except Exception as e:
            print(f"  âŒ ç„¡æ³•å‰µå»ºæ¨¡å‹ {model_name} çš„æ··æ·†çŸ©é™£åœ–è¡¨: {e}")
    
    def _generate_model_performance_chart(self, model_name: str, result: Dict):
        """
        ç‚ºç‰¹å®šæ¨¡å‹ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™åœ–è¡¨
        
        Args:
            model_name: æ¨¡å‹åç¨±
            result: è©•ä¼°çµæœå­—å…¸
        """
        try:
            if 'individual_recalls' not in result:
                return
                
            # æº–å‚™æ•¸æ“š
            metrics = {
                'Accuracy': result.get('accuracy', 0.0),
                'Macro Recall': result.get('macro_recall', 0.0),
                'Weighted Recall': result.get('weighted_recall', 0.0)
            }
            
            # æ·»åŠ å„é¡åˆ¥recall
            for class_name, recall_val in result['individual_recalls'].items():
                metrics[f'{class_name} Recall'] = recall_val
            
            # å‰µå»ºåœ–è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))
            
            metric_names = list(metrics.keys())
            metric_values = list(metrics.values())
            
            bars = ax.bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, value in zip(bars, metric_values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                       f'{value:.3f}', ha='center', va='bottom')
            
            ax.set_ylim(0, 1.1)
            ax.set_ylabel('Score')
            ax.set_title(f'Performance Metrics - {model_name}')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            
            # ä¿å­˜åœ–ç‰‡
            safe_model_name = model_name.replace('.pth', '').replace('/', '_').replace('\\', '_')
            performance_file = self.images_dir / f"performance_{safe_model_name}.png"
            
            plt.savefig(performance_file, dpi=300, bbox_inches='tight')
            print(f"  ğŸ–¼ï¸ æ€§èƒ½åœ–è¡¨å·²ä¿å­˜: {performance_file}")
            plt.close()
            
        except Exception as e:
            print(f"  âŒ ç„¡æ³•å‰µå»ºæ¨¡å‹ {model_name} çš„æ€§èƒ½åœ–è¡¨: {e}")
    
    def get_model_files(self) -> List[str]:
        """
        ç²å–modelsè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
        
        Returns:
            List[str]: æ¨¡å‹æª”æ¡ˆåç¨±åˆ—è¡¨
        """
        if not self.models_dir.exists():
            raise FileNotFoundError(f"æ¨¡å‹è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.models_dir}")
        
        # å°‹æ‰¾æ‰€æœ‰.pthæª”æ¡ˆå’Œç„¡å‰¯æª”åçš„æ¨¡å‹æª”æ¡ˆ
        model_files = []
        for file_path in self.models_dir.iterdir():
            if file_path.is_file():
                # åŒ…å«.pthæª”æ¡ˆå’Œçœ‹èµ·ä¾†åƒæ¨¡å‹çš„æª”æ¡ˆ
                if (file_path.suffix == '.pth' or 
                    ('model' in file_path.name.lower() and file_path.suffix == '')):
                    model_files.append(file_path.name)
        
        if not model_files:
            raise ValueError(f"åœ¨ {self.models_dir} ä¸­æ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ")
        
        print(f"æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹æª”æ¡ˆ:")
        for model_file in model_files:
            print(f"  - {model_file}")
        
        return sorted(model_files)
    
    def evaluate_single_model(self, model_name: str) -> Dict:
        """
        è©•ä¼°å–®ä¸€æ¨¡å‹
        
        Args:
            model_name (str): æ¨¡å‹æª”æ¡ˆåç¨±
            
        Returns:
            Dict: åŒ…å«è©•ä¼°çµæœçš„å­—å…¸
        """
        print(f"\nğŸ” æ­£åœ¨è©•ä¼°æ¨¡å‹: {model_name}")
        
        try:
            # ä½¿ç”¨ft_transformerçš„evaluate_modelæ–¹æ³•
            accuracy, confusion_matrix = self.ft_transformer.evaluate_model(model_name)
            
            # ç²å–è©³ç´°çš„åˆ†é¡å ±å‘Š
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # é‡æ–°ç²å–é æ¸¬çµæœä»¥è¨ˆç®—è©³ç´°æŒ‡æ¨™
            predictions = []
            true_labels = []
            
            self.ft_transformer.model.eval()
            with torch.no_grad():
                for batch in self.ft_transformer.dl_test:
                    numerical = batch['continuous'].to(device)
                    categorical = batch['categorical'].to(device)
                    target = batch['target'].to(device).squeeze()
                    outputs = self.ft_transformer.model({'continuous': numerical, 'categorical': categorical})
                    _, preds = torch.max(outputs['logits'], dim=1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(target.cpu().numpy())
            
            # è¨ˆç®—åˆ†é¡å ±å‘Šå­—å…¸æ ¼å¼
            class_report = classification_report(true_labels, predictions, output_dict=True)
            
            # æå–recallå€¼ (é‡å°æ¯å€‹é¡åˆ¥å’Œç¸½é«”)
            recalls = {}
            for key, metrics in class_report.items():
                if key not in ['accuracy', 'macro avg', 'weighted avg']:
                    recalls[f'class_{key}'] = metrics.get('recall', 0.0)
            
            # ä½¿ç”¨macro average recallä½œç‚ºä¸»è¦æŒ‡æ¨™
            macro_recall = class_report['macro avg']['recall']
            weighted_recall = class_report['weighted avg']['recall']
            
            result = {
                'model_name': model_name,
                'accuracy': accuracy,
                'macro_recall': macro_recall,
                'weighted_recall': weighted_recall,
                'individual_recalls': recalls,
                'confusion_matrix': confusion_matrix.tolist(),
                'full_classification_report': class_report
            }
            
            print(f"âœ… æ¨¡å‹ {model_name} è©•ä¼°å®Œæˆ")
            print(f"   æº–ç¢ºç‡: {accuracy:.4f}")
            print(f"   Macro Recall: {macro_recall:.4f}")
            print(f"   Weighted Recall: {weighted_recall:.4f}")
            
            # ç‚ºæ¨¡å‹ç”Ÿæˆæ··æ·†çŸ©é™£åœ–ç‰‡
            self._plot_confusion_matrix(confusion_matrix, ['Not Readmitted', 'Readmitted'], model_name)
            
            # ç‚ºæ¨¡å‹ç”Ÿæˆæ€§èƒ½åœ–è¡¨
            self._generate_model_performance_chart(model_name, result)
            
            return result
            
        except Exception as e:
            error_msg = f"è©•ä¼°æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                'model_name': model_name,
                'error': error_msg,
                'accuracy': 0.0,
                'macro_recall': 0.0,
                'weighted_recall': 0.0
            }
    
    def generate_model_comparison_chart(self):
        """
        ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ¯”è¼ƒåœ–è¡¨
        """
        if not self.evaluation_results:
            print("âš ï¸ å°šæœªé€²è¡Œæ¨¡å‹è©•ä¼°ï¼Œç„¡æ³•ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨")
            return
        
        try:
            # æº–å‚™æ•¸æ“š
            models = []
            accuracies = []
            macro_recalls = []
            weighted_recalls = []
            
            for model_name, result in self.evaluation_results.items():
                if 'error' not in result:  # åªåŒ…å«æˆåŠŸè©•ä¼°çš„æ¨¡å‹
                    # æ¸…ç†æ¨¡å‹åç¨±ç”¨æ–¼é¡¯ç¤º
                    display_name = model_name.replace('.pth', '')
                    models.append(display_name)
                    accuracies.append(result.get('accuracy', 0.0))
                    macro_recalls.append(result.get('macro_recall', 0.0))
                    weighted_recalls.append(result.get('weighted_recall', 0.0))
            
            if not models:
                print("âš ï¸ æ²’æœ‰æˆåŠŸè©•ä¼°çš„æ¨¡å‹ï¼Œç„¡æ³•ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨")
                return
            
            # å‰µå»ºå­åœ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
            
            # 1. æº–ç¢ºç‡æ¯”è¼ƒï¼ˆæŸ±ç‹€åœ–ï¼‰
            axes[0, 0].bar(models, accuracies, color='skyblue', alpha=0.7)
            axes[0, 0].set_title('Accuracy Comparison')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for i, v in enumerate(accuracies):
                axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            
            # 2. Macro Recallæ¯”è¼ƒï¼ˆæŸ±ç‹€åœ–ï¼‰
            bars = axes[0, 1].bar(models, macro_recalls, color='lightgreen', alpha=0.7)
            axes[0, 1].set_title('Macro Recall Comparison')
            axes[0, 1].set_ylabel('Macro Recall')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # çªå‡ºé¡¯ç¤ºæœ€ä½³æ¨¡å‹
            if self.best_recall_model:
                best_idx = None
                for i, model in enumerate(models):
                    if self.best_recall_model.replace('.pth', '') == model:
                        best_idx = i
                        break
                if best_idx is not None:
                    bars[best_idx].set_color('gold')
                    bars[best_idx].set_alpha(1.0)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for i, v in enumerate(macro_recalls):
                axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            
            # 3. Weighted Recallæ¯”è¼ƒï¼ˆæŸ±ç‹€åœ–ï¼‰
            axes[1, 0].bar(models, weighted_recalls, color='lightcoral', alpha=0.7)
            axes[1, 0].set_title('Weighted Recall Comparison')
            axes[1, 0].set_ylabel('Weighted Recall')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for i, v in enumerate(weighted_recalls):
                axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            
            # 4. é›·é”åœ–æ¯”è¼ƒï¼ˆæ‰€æœ‰æŒ‡æ¨™ï¼‰
            if len(models) <= 5:  # åªåœ¨æ¨¡å‹æ•¸é‡ä¸å¤šæ™‚é¡¯ç¤ºé›·é”åœ–
                ax = axes[1, 1]
                
                # æº–å‚™é›·é”åœ–æ•¸æ“š
                metrics = ['Accuracy', 'Macro Recall', 'Weighted Recall']
                angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                angles += angles[:1]  # é–‰åˆåœ–å½¢
                
                ax = plt.subplot(2, 2, 4, projection='polar')
                
                for i, model in enumerate(models):
                    values = [accuracies[i], macro_recalls[i], weighted_recalls[i]]
                    values += values[:1]  # é–‰åˆåœ–å½¢
                    
                    color = plt.cm.tab10(i)
                    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)
                    ax.fill(angles, values, alpha=0.1, color=color)
                
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(metrics)
                ax.set_ylim(0, 1)
                ax.set_title('Performance Radar Chart')
                ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            else:
                # å¦‚æœæ¨¡å‹å¤ªå¤šï¼Œé¡¯ç¤ºç°¡å–®çš„ç·šåœ–
                ax = axes[1, 1]
                x_pos = range(len(models))
                ax.plot(x_pos, accuracies, 'o-', label='Accuracy', linewidth=2)
                ax.plot(x_pos, macro_recalls, 's-', label='Macro Recall', linewidth=2)
                ax.plot(x_pos, weighted_recalls, '^-', label='Weighted Recall', linewidth=2)
                ax.set_title('Performance Trends')
                ax.set_ylabel('Score')
                ax.set_xticks(x_pos)
                ax.set_xticklabels(models, rotation=45)
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜æ¯”è¼ƒåœ–è¡¨
            comparison_file = self.images_dir / "model_comparison_chart.png"
            plt.savefig(comparison_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ–¼ï¸ æ¨¡å‹æ¯”è¼ƒåœ–è¡¨å·²ä¿å­˜: {comparison_file}")
            plt.close()
            
        except Exception as e:
            print(f"âŒ ç„¡æ³•å‰µå»ºæ¨¡å‹æ¯”è¼ƒåœ–è¡¨: {e}")
            import traceback
            traceback.print_exc()
    
    def evaluate_all_models(self, recall_metric: str = 'macro_recall') -> Dict[str, Dict]:
        """
        è©•ä¼°æ‰€æœ‰æ¨¡å‹
        
        Args:
            recall_metric (str): ä½¿ç”¨çš„recallæŒ‡æ¨™ ('macro_recall' æˆ– 'weighted_recall')
            
        Returns:
            Dict[str, Dict]: åŒ…å«æ‰€æœ‰æ¨¡å‹è©•ä¼°çµæœçš„å­—å…¸
        """
        print(f"\nğŸš€ é–‹å§‹è©•ä¼°æ‰€æœ‰æ¨¡å‹ï¼ˆä½¿ç”¨ {recall_metric} ä½œç‚ºæ¯”è¼ƒæŒ‡æ¨™ï¼‰...")
        
        model_files = self.get_model_files()
        
        # é‡ç½®çµæœ
        self.evaluation_results = {}
        self.best_recall_model = None
        self.best_recall_score = 0.0
        
        # è©•ä¼°æ¯å€‹æ¨¡å‹
        for model_file in model_files:
            result = self.evaluate_single_model(model_file)
            self.evaluation_results[model_file] = result
            
            # æ›´æ–°æœ€ä½³recallæ¨¡å‹
            current_recall = result.get(recall_metric, 0.0)
            if current_recall > self.best_recall_score:
                self.best_recall_score = current_recall
                self.best_recall_model = model_file
        
        print(f"\nğŸ¯ è©•ä¼°å®Œæˆï¼")
        print(f"æœ€ä½³æ¨¡å‹: {self.best_recall_model}")
        print(f"æœ€ä½³ {recall_metric}: {self.best_recall_score:.4f}")
        
        # ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒåœ–è¡¨
        self.generate_model_comparison_chart()
        
        return self.evaluation_results
    
    def get_best_model(self) -> Tuple[Optional[str], float]:
        """
        ç²å–recallæœ€é«˜çš„æ¨¡å‹
        
        Returns:
            Tuple[Optional[str], float]: (æœ€ä½³æ¨¡å‹åç¨±, æœ€ä½³recallåˆ†æ•¸)
        """
        return self.best_recall_model, self.best_recall_score
    
    def get_results_summary(self) -> pd.DataFrame:
        """
        ç²å–æ‰€æœ‰æ¨¡å‹çš„è©•ä¼°çµæœæ‘˜è¦
        
        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰æ¨¡å‹è©•ä¼°çµæœçš„DataFrame
        """
        if not self.evaluation_results:
            print("âš ï¸ å°šæœªé€²è¡Œæ¨¡å‹è©•ä¼°ï¼Œè«‹å…ˆåŸ·è¡Œ evaluate_all_models()")
            return pd.DataFrame()
        
        summary_data = []
        for model_name, result in self.evaluation_results.items():
            summary_data.append({
                'Model': model_name,
                'Accuracy': result.get('accuracy', 0.0),
                'Macro Recall': result.get('macro_recall', 0.0),
                'Weighted Recall': result.get('weighted_recall', 0.0),
                'Error': result.get('error', '')
            })
        
        df = pd.DataFrame(summary_data)
        # æŒ‰ç…§Macro Recallé™åºæ’åˆ—
        df = df.sort_values('Macro Recall', ascending=False).reset_index(drop=True)
        
        return df
    
    def save_results(self, output_file: str = 'model_evaluation_results.csv'):
        """
        å°‡è©•ä¼°çµæœä¿å­˜åˆ°CSVæª”æ¡ˆ
        
        Args:
            output_file (str): è¼¸å‡ºæª”æ¡ˆåç¨±
        """
        summary_df = self.get_results_summary()
        summary_df.to_csv(output_file, index=False)
        print(f"ğŸ“Š è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {output_file}")
    
    def print_detailed_results(self):
        """å°å‡ºè©³ç´°çš„è©•ä¼°çµæœ"""
        if not self.evaluation_results:
            print("âš ï¸ å°šæœªé€²è¡Œæ¨¡å‹è©•ä¼°")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š è©³ç´°è©•ä¼°çµæœ")
        print("="*80)
        
        # æŒ‰ç…§macro recallæ’åº
        sorted_results = sorted(
            self.evaluation_results.items(),
            key=lambda x: x[1].get('macro_recall', 0.0),
            reverse=True
        )
        
        for i, (model_name, result) in enumerate(sorted_results, 1):
            print(f"\n{i}. æ¨¡å‹: {model_name}")
            print(f"   æº–ç¢ºç‡: {result.get('accuracy', 0.0):.4f}")
            print(f"   Macro Recall: {result.get('macro_recall', 0.0):.4f}")
            print(f"   Weighted Recall: {result.get('weighted_recall', 0.0):.4f}")
            
            if 'individual_recalls' in result:
                print("   å„é¡åˆ¥Recall:")
                for class_name, recall_val in result['individual_recalls'].items():
                    print(f"     {class_name}: {recall_val:.4f}")
            
            if result.get('error'):
                print(f"   âŒ éŒ¯èª¤: {result['error']}")
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_recall_model}")
        print(f"ğŸ¯ æœ€ä½³Macro Recall: {self.best_recall_score:.4f}")


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # å‰µå»ºè©•ä¼°å™¨å¯¦ä¾‹
    evaluator = ModelEvaluator(
        models_dir="models",
        images_dir="images",
        use_smote=False,
        use_under_sampling=False
    )
    
    # è©•ä¼°æ‰€æœ‰æ¨¡å‹
    results = evaluator.evaluate_all_models(recall_metric='macro_recall')
    
    # ç²å–æœ€ä½³æ¨¡å‹
    best_model, best_score = evaluator.get_best_model()
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model}")
    print(f"ğŸ¯ æœ€ä½³Recallåˆ†æ•¸: {best_score:.4f}")
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\nğŸ“Š çµæœæ‘˜è¦:")
    summary_df = evaluator.get_results_summary()
    print(summary_df.to_string(index=False))
    
    # ä¿å­˜çµæœ
    evaluator.save_results('model_evaluation_results.csv')
    
    # é¡¯ç¤ºè©³ç´°çµæœ
    evaluator.print_detailed_results()
